# CH05 决策树

[TOC]

## 前言

### 章节目录

1. [模型与策略]决策树**模型**与**学习**
   1. 决策树模型
      1. 决策树与if-then规则
      1. 决策树与条件概率分布
   1. 决策树学习(三个步骤:特征选择，决策树生成，决策树修剪)
1. ~~算法~~
   1. [算法]特征选择
      1. 特征选择问题(下面两个介绍了常用的准则，另外还有基尼系数在CART算法部分讲解)
         1. 信息增益
         1. 信息增益比
   1. [算法]决策树的生成
      1. ID3算法
      1. C4.5的生成算法
   1. [算法]决策树的剪枝
   1. [算法]CART算法
      1. CART生成
      1. CART剪枝

### 导读

- 决策树是一种基本的分类与回归方法. 在书中CART算法之前的章节说的都是分类树，ID3和C4.5都只能处理分类问题，从CART(Classification and Regression Tree)开始有回归树，统称为决策树
- 以上在章节目录部分添加了一部分标记，把这个章节按照模型，策略与算法进行划分，进一步重新整理了结构，希望可以帮助理清章节内容之间的关系
- 在[CH12](../CH12/README.md)中有提到，决策树学习的损失函数是**对数似然损失**
- 这个章节的主题是决策树，内容内涵和外延都很广，这个章节推荐阅读图灵社区的一个访谈[^1]，了解一下李航老师的故事，也可以对本章的最后三个参考文献[^2][^3][^4]有进一步的了解.
- 引文中关于CART的介绍，是一本368页的书，2017年10月有了[Kindle版本](https://www.amazon.com/Classification-Regression-Trees-Leo-Breiman-ebook/dp/B076M7QKC6)，书的共同作者Friedman JH也是另一本神书ESL[参考文献7]的共同作者。
- CART虽然在本书中排在ID3和C4.5后面，但是发表的时间顺序为CART->ID3->C4.5，了解决策树历史可以参考Loh的报告[^5]

## 概念

### 熵

$$
H(p)=H(X)-\sum_{i=1}^{n}p_i\log p_i
$$

**熵只与$X$的分布有关，与$X$取值无关**，这句注意理解

### 条件熵

随机变量$(X,Y)$的联合概率分布为

$P(X=x_i,Y=y_j)=p_{ij}, i=1,2,\dots ,n;j=1,2,\dots ,m$

条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
其中$p_i=P(X=x_i),i=1,2,\dots ,n$

### 经验熵， 经验条件熵

> 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵

### 信息增益

特征$A$对训练数据集$D$的信息增益$g(D|A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差
$$
g(D,A)=H(D)-H(D|A)
$$


熵与条件熵的差称为互信息

## 算法

### 算法5.1 信息增益

> 输入：训练数据集$D$和特征$A$
>
> 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$
>
> 1. $H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$
> 1. $H(D|A)$
> 1. $g(D,A)$

### 算法5.2 ID3

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. $D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
> 1. $A$是空集->T为单节点树，实例数最多的作为该节点类标记，返回T
> 1. 计算$g$, 选择信息增益最大的特征$A_g$
> 1. 如果$A_g$的信息增益小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
> 1. $A_g$划分若干非空子集$D_i$，
> 1. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$



### 算法5.3 C4.5生成算法

> 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
> 输出：决策树$T$
>
> 1. D属于同一类C_k -> T为单节点树，类Ck作为该节点的类标记，返回T
> 1. A是空集->T为单节点树，实例数最多的作为该节点类标记，返回T
> 1. 计算g, 选择信息增益最大的特征Ag
> 1. 如果Ag的信息增益小于ε，T为单节点树，D中实例数最大的类Ck作为类标记，返回T
> 1. Ag划分若干非空子集Di，
> 1. Di训练集，A-{Ag}为特征集，递归调用前面步骤，得到Ti，返回Ti

### 算法5.4 树的剪枝算法

### 算法5.5 最小二乘回归树生成算法

### 算法5.6 CART生成算法

### 算法5.7 CART剪枝算法



## 例子

### 例5.1

这个例子引出在特征选择的问题，后面跟着引出了熵，条件熵，信息增益与信息增益比的概念。这些是介绍决策树学习的基础。

### 例5.2

根据信息增益准则选择最优特征



## 参考

1. [^1]: [图灵社区李航访谈](http://www.ituring.com.cn/article/196610)

1. [^2]: [山西健司]()

1. [^3]: [决策列表, Text classification using ESC-based stochastic decision lists]()

1. [^4]: [李航，安倍，CL文章，Generalizing case frames using a thesaurus and the MDL principle](http://www.aclweb.org/anthology/J98-2002)

1. [^5]: [A Brief History of Classification and Regression Trees](http://washstat.org/presentations/20150604/loh_slides.pdf)