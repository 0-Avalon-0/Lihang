# 索引

## 前言

有时候读书会卡住，也许只是我们看问题的角度问题。同样的问题，在书中不同的地方会有提到，或相关，或无关。这个文档类似书中的最后的索引， 会加入一些个人的理解。

每个内容单独一条

## 内容

### Gram矩阵

$P_{34}$，在感知机中第一次提到



### 凸优化

$P_{100}$

[CH07](CH07/README.md)

### 拉格朗日对偶性

$P_{225}$附录C



### KKT 条件

见附录C

### 经验

提到经验，说的都是和训练数据集相关的

### 对偶



感知机里面有提到，支持向量机里面有提到

### 分离超平面

$P_{26}, P_{102}$ 支持向量机里面也有

### 内积

$P_{25}, P_{78}, P_{117}$在感知机、逻辑回归、支持向量机里面都有用到

### 指示函数

$P_{40}, P_{}$

这个函数在不同的教材上有不同的表示方式，比如在深度学习中表示为$\mathbf 1_{condition}$

### $L_p$距离

$P_{38}$

### 启发式方法

$P_{57}$决策树学习通常采用启发式方法，得到的决策树是次最优的。



### 熵，条件熵

$P_{60}$在决策树中首先提到

$P_{80}$最大熵原理部分也有提到，并有引用到第五章中的内容

### 动态规划

$P_{67}$决策树的剪枝算法可以由一种动态规划的算法实现。

$P_{184}$维特比算法实际上是用动态规划求解隐马尔可夫模型预测问题，即用动态规划求概率最大路径。

### 贝叶斯估计

### 目标函数

$P_9$在经验风险最小化的策略或者结构风险最小化策略的情况下，经验或结构风险函数是最优化的目标函数

### 函数间隔

$P_{27},P_{97}$在[感知机](CH02/README.md)和[支持向量机](CH07/README.md)部分，都有函数间隔的概念，在[AdaBoost](CH08/README.md)部分，实际上也有间隔的概念在里面。