# 索引

## 前言

有时候读书会卡住，也许只是我们看问题的角度问题。同样的问题，在书中不同的地方会有提到，或相关，或无关。这个文档类似书中的最后的索引， 会加入一些个人的理解。

每个内容单独一条

## 内容

### Gram矩阵

$P_{34}$，在感知机中第一次提到



### 凸优化

$P_{100}$

[CH07](CH07/README.md)

### 拉格朗日对偶性

$P_{225}$附录C



### KKT 条件

见附录C

### 经验

提到经验，说的都是和训练数据集相关的

### 对偶



感知机里面有提到，支持向量机里面有提到

### 分离超平面

$P_{26}, P_{102}$ 支持向量机里面也有

### 内积

$P_{25}, P_{78}, P_{117}$在感知机、逻辑回归、支持向量机里面都有用到

### 指示函数

$P_{40}, P_{}$

这个函数在不同的教材上有不同的表示方式，比如在深度学习中表示为$\mathbf 1_{condition}$

### $L_p$距离

$P_{38}$

### 启发式方法

$P_{57}$决策树学习通常采用启发式方法，得到的决策树是次最优的。



### 熵，条件熵

$P_{60}$在决策树中首先提到

$P_{80}$最大熵原理部分也有提到，并有引用到第五章中的内容

$P_{166}$ $F$函数的定义中,有定义分布$\hat P(Z)$的熵

### 动态规划

$P_{67}$决策树的剪枝算法可以由一种动态规划的算法实现。

$P_{184}$维特比算法实际上是用动态规划求解隐马尔可夫模型预测问题，即用动态规划求概率最大路径。

### 贝叶斯估计

### 目标函数

$P_9$在经验风险最小化的策略或者结构风险最小化策略的情况下，经验或结构风险函数是最优化的目标函数

### 函数间隔

$P_{27},P_{97}$在[感知机](CH02/README.md)和[支持向量机](CH07/README.md)部分，都有函数间隔的概念，在[AdaBoost](CH08/README.md)部分，实际上也有间隔的概念在里面。

### 概率分布密度

$P_{162}$ 高斯分布密度, 书中的内容扩展下去看二维混合高斯模型, 对协方差矩阵的理解会有帮助.

### 对数似然损失

$P_7$ 对数损失函数或者对数似然损失函数 $L(Y,P(Y|X))=-\log P(Y|X)$

### 对数似然函数

$P_{158}$ 面对一个含有隐变量的概率模型, 目标是极大化观测数据(不完全数据)Y关于参数$\theta$的对数似然函数, 即极大化
$$
\begin{aligned}L(\theta)=&\log P(Y|\theta)=\log \sum_Z P(Y, Z|\theta) \\
=&\log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)
\end{aligned}
$$

### One-hot Encoding

$P_{163}$ 注意这里书中没有明确的说明$\gamma_j$是One-hot encoding, 也叫做1-of-K representation

$\gamma_j=\sum_{k=1}^K\gamma_{jk}=1, j=1,2,3,\dots, n$



### 基函数

$P_{144}$

### 基本分类器

$P_{147}$ 上面这两个不是一个概念

### 琴声不等式

$P_{159}$ 