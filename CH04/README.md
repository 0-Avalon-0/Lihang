# CH04 朴素贝叶斯法

[TOC]

## 前言

### 章节目录

1. 朴素贝叶斯法的学习与分类
   1. 基本方法
   1. 后验概率最大化的含义
1. 朴素贝叶斯法的参数估计
   1. 极大似然估计
   1. 学习与分类算法
   1. 贝叶斯估计

### 导读

- 书中生成模型不多，NB是典型的生成模型
- **0-1损失函数**时的期望风险最小化
- 



## 朴素贝叶斯法

朴素贝叶斯法是基于**贝叶斯定理**与**特征条件独立假设**的分类方法.

- 贝叶斯定理
- 特征条件独立假设

条件独立性假设是:
$$
\begin{align}
P(X=x|Y=c_k)&=P(X^{(1)},\dots,X^{(n)}|Y=c_k)\\
&=\prod^n_{j=1}P(X^{(j)}=x^{(j)}|Y=c_k)
\end{align}
$$
条件独立假设等于是说用于分类的**特征**在**类确定**的条件下都是**条件独立**的.

## 参数估计

### 极大似然估计

> 为了估计状态变量的条件分布, 利用贝叶斯法则, 有
> $$
> P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}
> $$
> 其中$P(X|Y)$为后验概率(Posterior), $P(Y|X)$称为似然, $P(Y)$称为先验(Prior)[^1].
>

- 后验概率最大化的含义

  朴素贝叶斯法将实例分到**后验概率最大的类**中, 这等价于**期望风险最小化**. 

### 贝叶斯估计

对于x的某个特征的取值没有在先验中出现的情况, 如果用极大似然估计, 这种情况的可能性就是0.
但是出现这种情况的原因通常是因为数据集不能全覆盖样本空间, 出现未知的情况处理的策略就是做平滑.
(4.10)对应了出现未知样本的情况下, 该给出一个什么样的值才合理的方案.
$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum\limits_{i=1}^NI(x_i^{j}=a_{jl},y_j=c_k)+\lambda}{\sum\limits_{i=1}^NI(y_j=c_k)+S_j\lambda}
$$


其中$\lambda \geqslant 0$

当$\lambda = 0$的时候, 就是极大似然估计.

当$\lambda=1$的时候, 这个平滑方案叫做Laplace Smoothing. 拉普拉斯平滑相当于给未知变量给定了先验概率.

## 贝叶斯网

朴素贝叶斯法中假设输入变量都是条件独立的, 如果假设他们之间存在概率依存关系, 模型就变成了**贝叶斯网络**.

## 参考

1. [^1]: [视觉SLAM十四讲, 高翔](## 参考)