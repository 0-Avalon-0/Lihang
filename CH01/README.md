# CH01 统计学习方法概论

统计学习方法三要素:模型,策略,算法.

## 实现统计学习方法的步骤
1. 得到一个有限的训练数据集合
1. 确定包含所有可能的模型的**假设空间**, 即学习模型的集合.
1. 确定模型选择的准则, 即学习的**策略**
1. 实现求解最优模型的算法, 即学习的**算法**
1. 通过学习方法选择最优的模型
1. 利用学习的最优模型对新数据进行预测或分析.

## 模型是什么?
在监督学习过程中, 模型就是所要学习的**条件概率分布**或者**决策函数**.

## 损失函数与风险函数

1. 损失函数(loss function)或代价函数(cost function)
   损失函数定义为给定输入$X$的预测值$f(X)$和真实值$Y$之间的非负实值函数, 记作$L(Y,f(X))$

1. 风险函数(risk function)或期望损失(expected loss)
   $R_{exp}(f)=E_p[L(Y, f(X))]=\int_{\mathcal X\times\mathcal Y}L(y,f(x))P(x,y)\, {\rm d}x{\rm d}y$
   模型$f(X)$关于联合分布$P(X,Y)$的**平均意义下的**损失(**期望**损失), 但是因为$P(X,Y)$是未知的, 所以前面的用词是**期望**, 以及**平均意义下的**.

   这个表示其实就是损失的均值, 反映了对整个数据的预测效果的好坏, $P(x,y)$转换成$\frac {\nu(X=x, Y=y)}{N}$更容易直观理解, 可以参考[CH9](../CH9/README.md), 6.2.2节的部分描述来理解, 但是真实的数据N是无穷的.

1. **经验风险**(empirical risk)或**经验损失**(empirical loss)
   $R_{emp}(f)=\frac{1}{N}\sum^{N}_{i=1}L(y_i,f(x_i))$
   模型$f$关于训练样本集的平均损失
   根据大数定律, 当样本容量N趋于无穷大时, 经验风险趋于期望风险.

1. **结构风险**(structural risk)
   $R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))+\lambda J(f)$
   $J(f)$为模型复杂度, $\lambda \geqslant 0$是系数, 用以权衡经验风险和模型复杂度.

## 经验风险最小化与结构风险最小化

 

1. **极大似然估计**是经验风险最小化的一个例子.
   当模型是条件概率分布, 损失函数是对数损失函数时, 经验风险最小化等价于极大似然估计.
1. **贝叶斯估计**中的**最大后验概率估计**是结构风险最小化的一个例子.
   当模型是条件概率分布, 损失函数是对数损失函数, **模型复杂度由模型的先验概率表示**时, 结构风险最小化等价于最大后验概率估计.

## 模型选择

1. 正则化
   模型选择的典型方法是正则化
1. 交叉验证
   另一种常用的模型选择方法是交叉验证
   - 简单
   - S折(K折, K-Fold)[^1]
   - 留一法



## 生成模型与判别模型

监督学习方法可分为**生成方法**(generative approach)与**判别方法**(discriminative approach)

### 生成模型

generative model

- 可以还原出**联合概率分布**$P(X,Y)$
- 收敛速度快, 当样本容量增加时, 学到的模型可以更快收敛到真实模型
- 当存在隐变量时仍可以用

### 判别方法

discriminative model

- 直接学习**条件概率**$P(Y|X)$或者**决策函数**$f(X)$
- 直接面对预测, 往往学习准确率更高
- 可以对数据进行各种程度的抽象,  定义特征并使用特征, 可以简化学习问题

## 参考

1. [^1]: [ESL:7.10.1:K-Forld Cross Validation](##参考)